{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "133bc118",
   "metadata": {},
   "source": [
    "# Assignment 3 group 25"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53f321d8",
   "metadata": {},
   "source": [
    "By:\n",
    "Alexandra Bock, Elena Ecker, Alessandro Maugeri, Giorgio Rinaldi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cba11198",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Utente\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Utente\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import date, datetime\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.probability import FreqDist\n",
    "from nltk import punkt\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from sklearn.metrics import adjusted_rand_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "from sklearn import svm\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e1f47ab",
   "metadata": {},
   "source": [
    "## Extraction and Manipulation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9afd1463",
   "metadata": {},
   "source": [
    "Here are represented the historical data of Tesla stock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c1079312",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              Date Price  Open  High   Low   Vol. Change %\n",
      "2484  Jan 03, 2012  5.62  5.79  5.90  5.53  4.64M   -1.58%\n",
      "2483  Jan 04, 2012  5.54  5.64  5.73  5.50  3.15M   -1.42%\n",
      "2482  Jan 05, 2012  5.42  5.55  5.59  5.37  5.03M   -2.17%\n",
      "2481  Jan 06, 2012  5.38  5.44  5.56  5.28  4.93M   -0.74%\n",
      "2480  Jan 09, 2012  5.45  5.40  5.50  5.22  4.48M    1.30%\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2485 entries, 2484 to 0\n",
      "Data columns (total 7 columns):\n",
      " #   Column    Non-Null Count  Dtype \n",
      "---  ------    --------------  ----- \n",
      " 0   Date      2485 non-null   object\n",
      " 1   Price     2485 non-null   object\n",
      " 2   Open      2485 non-null   object\n",
      " 3   High      2485 non-null   object\n",
      " 4   Low       2485 non-null   object\n",
      " 5   Vol.      2485 non-null   object\n",
      " 6   Change %  2485 non-null   object\n",
      "dtypes: object(7)\n",
      "memory usage: 136.0+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "tesla = pd.read_csv('TSLA Historical Data.csv')[::-1]\n",
    "print(tesla.head())\n",
    "print(tesla.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3d7fe63",
   "metadata": {},
   "source": [
    "We are getting rid of useless information and resorting the dataframe by date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "80b3f823",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Change %        date\n",
      "0   -1.58%  2012-01-03\n",
      "1   -1.42%  2012-01-04\n",
      "2   -2.17%  2012-01-05\n",
      "3   -0.74%  2012-01-06\n",
      "4    1.30%  2012-01-09\n"
     ]
    }
   ],
   "source": [
    "tesla['date']= pd.to_datetime(tesla['Date']).dt.date\n",
    "tesla = tesla.drop(axis=1, columns=['Price', 'Open', 'High', 'Low', 'Vol.', 'Date'])\n",
    "tesla = tesla.sort_values(by='date').reset_index(drop=True)\n",
    "print(tesla.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "909eecbe",
   "metadata": {},
   "source": [
    "Elon tweets since 2012"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d558502e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   username   to                                               text  retweets  \\\n",
      "0  elonmusk  NaN  Just test-fired the Superdraco rocket engine. ...        63   \n",
      "1  elonmusk  NaN  Um, while awesome in its own way, that was not...         5   \n",
      "2  elonmusk  NaN  The SpaceX theme song by Total Ghost is just t...        32   \n",
      "3  elonmusk  NaN  The lady doth protest too little. http://finan...         4   \n",
      "4  elonmusk  NaN  The Model X unveiling will be webcast live on ...       109   \n",
      "\n",
      "   favorites  replies                  id  \\\n",
      "0         67       13  164740558374445056   \n",
      "1         19       12  164637208769339392   \n",
      "2         79       14  164389972688908289   \n",
      "3          2        1  163349291790893056   \n",
      "4         23       18  163084140386861056   \n",
      "\n",
      "                                           permalink  author_id  \\\n",
      "0  https://twitter.com/elonmusk/status/1647405583...   44196397   \n",
      "1  https://twitter.com/elonmusk/status/1646372087...   44196397   \n",
      "2  https://twitter.com/elonmusk/status/1643899726...   44196397   \n",
      "3  https://twitter.com/elonmusk/status/1633492917...   44196397   \n",
      "4  https://twitter.com/elonmusk/status/1630841403...   44196397   \n",
      "\n",
      "                        date                  formatted_date hashtags  \\\n",
      "0  2012-02-01 16:03:05+00:00  Wed Feb 01 16:03:05 +0000 2012      NaN   \n",
      "1  2012-02-01 09:12:24+00:00  Wed Feb 01 09:12:24 +0000 2012      NaN   \n",
      "2  2012-01-31 16:49:58+00:00  Tue Jan 31 16:49:58 +0000 2012      NaN   \n",
      "3  2012-01-28 19:54:40+00:00  Sat Jan 28 19:54:40 +0000 2012      NaN   \n",
      "4  2012-01-28 02:21:03+00:00  Sat Jan 28 02:21:03 +0000 2012      NaN   \n",
      "\n",
      "  mentions  geo                                               urls  \n",
      "0      NaN  NaN                                                NaN  \n",
      "1      NaN  NaN                                                NaN  \n",
      "2      NaN  NaN         http://www.youtube.com/watch?v=MezkEiS-6jA  \n",
      "3      NaN  NaN  http://finance.yahoo.com/news/topless-proteste...  \n",
      "4      NaN  NaN                             http://teslamotors.com  \n"
     ]
    }
   ],
   "source": [
    "data_url = 'https://raw.githubusercontent.com/NuttySalmon/Elon-Musk-Tweets-VS-Tesla-Stock/master/tweets.csv'\n",
    "elon = pd.read_csv(data_url)\n",
    "print(elon.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b432b94",
   "metadata": {},
   "source": [
    "Getting rid of useless information about tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d169e3a2",
   "metadata": {},
   "source": [
    "First goal: transform the types of data in order to process them. First let's analyze the the info about this csv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9b2bd0e7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 9407 entries, 0 to 9406\n",
      "Data columns (total 15 columns):\n",
      " #   Column          Non-Null Count  Dtype  \n",
      "---  ------          --------------  -----  \n",
      " 0   username        9407 non-null   object \n",
      " 1   to              6717 non-null   object \n",
      " 2   text            9314 non-null   object \n",
      " 3   retweets        9407 non-null   int64  \n",
      " 4   favorites       9407 non-null   int64  \n",
      " 5   replies         9407 non-null   int64  \n",
      " 6   id              9407 non-null   int64  \n",
      " 7   permalink       9407 non-null   object \n",
      " 8   author_id       9407 non-null   int64  \n",
      " 9   date            9407 non-null   object \n",
      " 10  formatted_date  9407 non-null   object \n",
      " 11  hashtags        42 non-null     object \n",
      " 12  mentions        808 non-null    object \n",
      " 13  geo             0 non-null      float64\n",
      " 14  urls            1251 non-null   object \n",
      "dtypes: float64(1), int64(5), object(9)\n",
      "memory usage: 1.1+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(elon.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "794d13be",
   "metadata": {},
   "source": [
    "Text will be treated as string, date as date (we are removing the time). adding day, month, year columns. resorting by date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "caf4774b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text        date\n",
      "0  Just finished Massie's book on Catherine. An a...  2012-01-03\n",
      "1  What everyone really thinks ... RT “@jonlovett...  2012-01-03\n",
      "2  @om Respectfully disagree. Larry made awesome ...  2012-01-03\n",
      "3  Next month is also when our Dragon spaceship d...  2012-01-06\n",
      "4      Model S Signature series sold out as of today  2012-01-06\n"
     ]
    }
   ],
   "source": [
    "elon = elon.drop(axis=1, columns=['retweets','favorites','replies','to', 'hashtags','author_id','permalink','mentions','geo','urls','id','username', 'formatted_date'])\n",
    "elon['text'] = elon['text'].astype('string')\n",
    "elon['date']= pd.to_datetime(elon['date']).dt.date\n",
    "elon = elon.sort_values(by='date').reset_index(drop=True)\n",
    "print(elon.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e87b4c6e",
   "metadata": {},
   "source": [
    "Merge the two datasets linking by the same date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "605c1a97",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "merge_df = pd.merge(tesla, elon,  how='left', left_on=['date'], right_on = ['date'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dc0cfed",
   "metadata": {},
   "source": [
    "The 'date' column now is no more usefull since we've the correspondence between the change in stock and the text of each tweet in the corresponding day. These two columns represent our X and Y, but first need to be manipulated.\n",
    "In particular, we remove all the days in which there were no tweets by Musk (dropna)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "00f6c9e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Change %                                               text\n",
      "0   -1.58%  Just finished Massie's book on Catherine. An a...\n",
      "1   -1.58%  What everyone really thinks ... RT “@jonlovett...\n",
      "2   -1.58%  @om Respectfully disagree. Larry made awesome ...\n",
      "5   -0.74%  Next month is also when our Dragon spaceship d...\n",
      "6   -0.74%      Model S Signature series sold out as of today\n"
     ]
    }
   ],
   "source": [
    "elon = merge_df.drop(axis=1, columns=['date']).dropna(axis=0)\n",
    "print(elon.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42861286",
   "metadata": {},
   "source": [
    "With lambda function applied to the 'Change %', positive trend will be classified as 1 and negative with -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9e629fb9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Change %                                               text Label\n",
      "7361  -10.31%                                            Exactly    -1\n",
      "7362  -10.31%                                          Well said    -1\n",
      "7363  -10.31%                                                       -1\n",
      "7364  -10.31%    Oh, say does that star-spangled banner yet wave    -1\n",
      "7365  -10.31%  Don’t need the cash. Devoting myself to Mars a...    -1\n"
     ]
    }
   ],
   "source": [
    "elon['Label'] = elon['Change %'].apply(lambda x: '1' if '-' not in x  else '-1')\n",
    "print(elon.tail())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a671ed5",
   "metadata": {},
   "source": [
    "Even here, we apply lambda to tokenize all the rows of text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7be6bb53",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Change %                                               text Label  \\\n",
      "0   -1.58%  Just finished Massie's book on Catherine. An a...    -1   \n",
      "1   -1.58%  What everyone really thinks ... RT “@jonlovett...    -1   \n",
      "2   -1.58%  @om Respectfully disagree. Larry made awesome ...    -1   \n",
      "5   -0.74%  Next month is also when our Dragon spaceship d...    -1   \n",
      "6   -0.74%      Model S Signature series sold out as of today    -1   \n",
      "\n",
      "                                     tokenized_sents  \n",
      "0  [Just, finished, Massie, 's, book, on, Catheri...  \n",
      "1  [What, everyone, really, thinks, ..., RT, “, @...  \n",
      "2  [@, om, Respectfully, disagree, ., Larry, made...  \n",
      "5  [Next, month, is, also, when, our, Dragon, spa...  \n",
      "6  [Model, S, Signature, series, sold, out, as, o...  \n"
     ]
    }
   ],
   "source": [
    "elon['tokenized_sents'] = elon.apply(lambda row: nltk.word_tokenize(row['text']), axis=1)\n",
    "print(elon.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50e3486a",
   "metadata": {},
   "source": [
    "Lemmatization required the creation of a function since we had to iterate over each element of a cell.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1f3dd984",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       [Just, finished, Massie, 's, book, on, Catheri...\n",
      "1       [What, everyone, really, think, ..., RT, “, @,...\n",
      "2       [@, om, Respectfully, disagree, ., Larry, made...\n",
      "5       [Next, month, is, also, when, our, Dragon, spa...\n",
      "6       [Model, S, Signature, series, sold, out, a, of...\n",
      "                              ...                        \n",
      "7361                                            [Exactly]\n",
      "7362                                         [Well, said]\n",
      "7363                                                   []\n",
      "7364    [Oh, ,, say, doe, that, star-spangled, banner,...\n",
      "7365    [Don, ’, t, need, the, cash, ., Devoting, myse...\n",
      "Name: lemmatize_sents, Length: 6395, dtype: object\n"
     ]
    }
   ],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "def lemmatize_text(text):\n",
    "    return [lemmatizer.lemmatize(w) for w in text]\n",
    "elon['lemmatize_sents'] = elon['tokenized_sents'].apply(lemmatize_text)\n",
    "print(elon['lemmatize_sents'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a2d63a8",
   "metadata": {},
   "source": [
    "We extract the positive and negative tweets and transform them to lists.\n",
    "We apply a function to remove punctuation and stop words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1d039c75",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['just finished massie book catherine an amazingly expansive compelling portrait incredible woman highly recommend', \"what everyone really think ... rt jonlovett if mayans good predicting future 'd mayans\", 'om respectfully disagree larry made awesome move last year laid foundation g+ kick butt 2012 he stud', 'next month also dragon spaceship dock space station first time major pucker factor ...', 'model signature series sold today'] ['the exec conf room tesla used called denali decided move letter around seemed apt', 'model performance powertrain produced much torque today broke dyno 4.4 sec 0-60 mph problem', \"kids day grown rt `` second-graders wow audience with school production of equus ''\", 'what magical cowboy herding cat livin dream ...', 'tsla 500m friday back 500m today sorry roller coaster']\n"
     ]
    }
   ],
   "source": [
    "negative_post = elon[elon['Label'] == '-1']\n",
    "negative_post = negative_post['lemmatize_sents'].copy()\n",
    "negative_post = negative_post.to_list()\n",
    "\n",
    "\n",
    "\n",
    "positive_post = elon[elon['Label'] == '1']\n",
    "positive_post = positive_post['lemmatize_sents'].copy()\n",
    "positive_post = positive_post.to_list()\n",
    "\n",
    "punctuation_stop = ['\\'s','.','(', ')', ',', ':', ';', '’','’s', '@', '#', '!', '?', '&','i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n",
    "\n",
    "def remove_p_s(x):\n",
    "    x1 = list()\n",
    "    for y in x:\n",
    "        x2 = []\n",
    "        for z in y:\n",
    "            if z not in punctuation_stop and len(z) > 1:\n",
    "                x2.append(z.lower())\n",
    "        x1.append(x2)\n",
    "    return x1            \n",
    "         \n",
    "negative_post = remove_p_s(negative_post)\n",
    "positive_post = remove_p_s(positive_post)\n",
    "\n",
    "\n",
    "negative_post = [\" \".join(x) for x in negative_post]\n",
    "positive_post = [\" \".join(x) for x in positive_post]\n",
    "print(negative_post[:5], positive_post[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6987c3f5",
   "metadata": {},
   "source": [
    "## Modelling and Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e87f039c",
   "metadata": {},
   "source": [
    "We now apply the td-idf technique to extract information about words. We will get a matrix in which as rows we have the single tweets and as columns all the unique words in all tweets.\n",
    "We then append our y, label +1 since it is positive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "29ef15e3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      000  000 km  000 lb  000 people   02   04   07   09   10  10 year  ...  \\\n",
      "0     0.0     0.0     0.0         0.0  0.0  0.0  0.0  0.0  0.0      0.0  ...   \n",
      "1     0.0     0.0     0.0         0.0  0.0  0.0  0.0  0.0  0.0      0.0  ...   \n",
      "2     0.0     0.0     0.0         0.0  0.0  0.0  0.0  0.0  0.0      0.0  ...   \n",
      "3     0.0     0.0     0.0         0.0  0.0  0.0  0.0  0.0  0.0      0.0  ...   \n",
      "4     0.0     0.0     0.0         0.0  0.0  0.0  0.0  0.0  0.0      0.0  ...   \n",
      "...   ...     ...     ...         ...  ...  ...  ...  ...  ...      ...  ...   \n",
      "3460  0.0     0.0     0.0         0.0  0.0  0.0  0.0  0.0  0.0      0.0  ...   \n",
      "3461  0.0     0.0     0.0         0.0  0.0  0.0  0.0  0.0  0.0      0.0  ...   \n",
      "3462  0.0     0.0     0.0         0.0  0.0  0.0  0.0  0.0  0.0      0.0  ...   \n",
      "3463  0.0     0.0     0.0         0.0  0.0  0.0  0.0  0.0  0.0      0.0  ...   \n",
      "3464  0.0     0.0     0.0         0.0  0.0  0.0  0.0  0.0  0.0      0.0  ...   \n",
      "\n",
      "      youtube com watch  yup   ze  ze ev  zero  zero gravity  zeroth  \\\n",
      "0                   0.0  0.0  0.0    0.0   0.0           0.0     0.0   \n",
      "1                   0.0  0.0  0.0    0.0   0.0           0.0     0.0   \n",
      "2                   0.0  0.0  0.0    0.0   0.0           0.0     0.0   \n",
      "3                   0.0  0.0  0.0    0.0   0.0           0.0     0.0   \n",
      "4                   0.0  0.0  0.0    0.0   0.0           0.0     0.0   \n",
      "...                 ...  ...  ...    ...   ...           ...     ...   \n",
      "3460                0.0  0.0  0.0    0.0   0.0           0.0     0.0   \n",
      "3461                0.0  0.0  0.0    0.0   0.0           0.0     0.0   \n",
      "3462                0.0  0.0  0.0    0.0   0.0           0.0     0.0   \n",
      "3463                0.0  0.0  0.0    0.0   0.0           0.0     0.0   \n",
      "3464                0.0  0.0  0.0    0.0   0.0           0.0     0.0   \n",
      "\n",
      "      zeroth law  zone  Label  \n",
      "0            0.0   0.0      1  \n",
      "1            0.0   0.0      1  \n",
      "2            0.0   0.0      1  \n",
      "3            0.0   0.0      1  \n",
      "4            0.0   0.0      1  \n",
      "...          ...   ...    ...  \n",
      "3460         0.0   0.0      1  \n",
      "3461         0.0   0.0      1  \n",
      "3462         0.0   0.0      1  \n",
      "3463         0.0   0.0      1  \n",
      "3464         0.0   0.0      1  \n",
      "\n",
      "[3465 rows x 4820 columns]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "tfidf = TfidfVectorizer(min_df=2,max_df=0.5, ngram_range=(1,3))\n",
    "features = tfidf.fit_transform(positive_post)\n",
    "df_Positive = pd.DataFrame(\n",
    "    features.todense(),\n",
    "    columns=tfidf.get_feature_names()\n",
    ")\n",
    "df_Positive['Label'] = '1'\n",
    "print(df_Positive)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30ce2193",
   "metadata": {},
   "source": [
    "Here the opposite, -1 since the values were negative durign the days tweets were posted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "58e8cb25",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      000  000 km  000 lb  000 mile  000 people   01   03   05   07   10  ...  \\\n",
      "0     0.0     0.0     0.0       0.0         0.0  0.0  0.0  0.0  0.0  0.0  ...   \n",
      "1     0.0     0.0     0.0       0.0         0.0  0.0  0.0  0.0  0.0  0.0  ...   \n",
      "2     0.0     0.0     0.0       0.0         0.0  0.0  0.0  0.0  0.0  0.0  ...   \n",
      "3     0.0     0.0     0.0       0.0         0.0  0.0  0.0  0.0  0.0  0.0  ...   \n",
      "4     0.0     0.0     0.0       0.0         0.0  0.0  0.0  0.0  0.0  0.0  ...   \n",
      "...   ...     ...     ...       ...         ...  ...  ...  ...  ...  ...  ...   \n",
      "2925  0.0     0.0     0.0       0.0         0.0  0.0  0.0  0.0  0.0  0.0  ...   \n",
      "2926  0.0     0.0     0.0       0.0         0.0  0.0  0.0  0.0  0.0  0.0  ...   \n",
      "2927  0.0     0.0     0.0       0.0         0.0  0.0  0.0  0.0  0.0  0.0  ...   \n",
      "2928  0.0     0.0     0.0       0.0         0.0  0.0  0.0  0.0  0.0  0.0  ...   \n",
      "2929  0.0     0.0     0.0       0.0         0.0  0.0  0.0  0.0  0.0  0.0  ...   \n",
      "\n",
      "      youtu  youtu be  youtube  youtube com  youtube com watch  yup  zero  \\\n",
      "0       0.0       0.0      0.0          0.0                0.0  0.0   0.0   \n",
      "1       0.0       0.0      0.0          0.0                0.0  0.0   0.0   \n",
      "2       0.0       0.0      0.0          0.0                0.0  0.0   0.0   \n",
      "3       0.0       0.0      0.0          0.0                0.0  0.0   0.0   \n",
      "4       0.0       0.0      0.0          0.0                0.0  0.0   0.0   \n",
      "...     ...       ...      ...          ...                ...  ...   ...   \n",
      "2925    0.0       0.0      0.0          0.0                0.0  0.0   0.0   \n",
      "2926    0.0       0.0      0.0          0.0                0.0  0.0   0.0   \n",
      "2927    0.0       0.0      0.0          0.0                0.0  0.0   0.0   \n",
      "2928    0.0       0.0      0.0          0.0                0.0  0.0   0.0   \n",
      "2929    0.0       0.0      0.0          0.0                0.0  0.0   0.0   \n",
      "\n",
      "      zip  zone  Label  \n",
      "0     0.0   0.0     -1  \n",
      "1     0.0   0.0     -1  \n",
      "2     0.0   0.0     -1  \n",
      "3     0.0   0.0     -1  \n",
      "4     0.0   0.0     -1  \n",
      "...   ...   ...    ...  \n",
      "2925  0.0   0.0     -1  \n",
      "2926  0.0   0.0     -1  \n",
      "2927  0.0   0.0     -1  \n",
      "2928  0.0   0.0     -1  \n",
      "2929  0.0   0.0     -1  \n",
      "\n",
      "[2930 rows x 4121 columns]\n"
     ]
    }
   ],
   "source": [
    "tfidf = TfidfVectorizer(min_df=2,max_df=0.5, ngram_range=(1,3))\n",
    "features = tfidf.fit_transform(negative_post)\n",
    "df_Negative = pd.DataFrame(\n",
    "    features.todense(),\n",
    "    columns=tfidf.get_feature_names()\n",
    ")\n",
    "df_Negative['Label'] = '-1'\n",
    "df_Negative.head()\n",
    "\n",
    "\n",
    "print(df_Negative)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a178c355",
   "metadata": {},
   "source": [
    "Now we get positive and negative reviews back together, and we transform types in np.float32 in order to process them in order to satisfy the needs of the algorithms we are going to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "93412369",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_Positive.append(df_Negative)\n",
    "df = df.fillna(0.0).astype(np.float32)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5896f074",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((6395, 6565), (6395,))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = df.drop('Label',axis=1)\n",
    "y = df['Label']\n",
    "\n",
    "x.shape, y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0db5066c",
   "metadata": {},
   "source": [
    "Training and test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2d91fc73",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    x, y, test_size = 0.2, random_state = 0, stratify = y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7a3bb1e",
   "metadata": {},
   "source": [
    "Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5b600f18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def runRandomForest(x_train, x_test, y_train, y_test):\n",
    "    clf = RandomForestClassifier(n_estimators=100, random_state=0, n_jobs=-1)\n",
    "    clf.fit(x_train, y_train)\n",
    "    y_pred = clf.predict(x_test)\n",
    "    cr = classification_report(y_test, y_pred)\n",
    "    print(cr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e8a63c6",
   "metadata": {},
   "source": [
    "SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4c9da725",
   "metadata": {},
   "outputs": [],
   "source": [
    "def SVC(x_train, x_test, y_train, y_test): \n",
    "    clf = svm.SVC(kernel='linear') # Linear Kernel\n",
    "#Train the model using the training sets\n",
    "    clf.fit(x_train, y_train)\n",
    "    y_pred = clf.predict(x_test)\n",
    "    cr = classification_report(y_test, y_pred)\n",
    "    print(cr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93a655e3",
   "metadata": {},
   "source": [
    "LSVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3500a933",
   "metadata": {},
   "outputs": [],
   "source": [
    "def LSVC(x_train, x_test, y_train, y_test):\n",
    "    lsvc = LinearSVC(verbose=0)\n",
    "    print(lsvc)\n",
    "\n",
    "    LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,\n",
    "              intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
    "              multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
    "              verbose=0)\n",
    "\n",
    "\n",
    "    lsvc.fit(x_train, y_train)\n",
    "    y_pred = lsvc.predict(x_test)\n",
    "    cr = classification_report(y_test, y_pred)\n",
    "    print(cr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9afb3d24",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "007a8984",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        -1.0       0.51      0.33      0.40       586\n",
      "         1.0       0.56      0.73      0.63       693\n",
      "\n",
      "    accuracy                           0.55      1279\n",
      "   macro avg       0.53      0.53      0.52      1279\n",
      "weighted avg       0.54      0.55      0.53      1279\n",
      "\n",
      "LinearSVC()\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        -1.0       0.60      0.54      0.57       586\n",
      "         1.0       0.64      0.70      0.67       693\n",
      "\n",
      "    accuracy                           0.63      1279\n",
      "   macro avg       0.62      0.62      0.62      1279\n",
      "weighted avg       0.63      0.63      0.63      1279\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        -1.0       0.58      0.44      0.50       586\n",
      "         1.0       0.61      0.73      0.66       693\n",
      "\n",
      "    accuracy                           0.60      1279\n",
      "   macro avg       0.59      0.58      0.58      1279\n",
      "weighted avg       0.59      0.60      0.59      1279\n",
      "\n"
     ]
    }
   ],
   "source": [
    "runRandomForest(x_train, x_test, y_train, y_test)\n",
    "LSVC(x_train, x_test, y_train, y_test)\n",
    "SVC(x_train, x_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59800ba9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
